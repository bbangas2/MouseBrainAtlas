{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.path import Path\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.environ['REPO_DIR'] + '/utilities')\n",
    "from utilities2015 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class PolygonType(Enum):\n",
    "    CLOSED = 'closed'\n",
    "    OPEN = 'open'\n",
    "    TEXTURE = 'textured'\n",
    "    TEXTURE_WITH_CONTOUR = 'texture with contour'\n",
    "    DIRECTION = 'directionality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.measure import find_contours\n",
    "\n",
    "from matplotlib.path import Path\n",
    "\n",
    "sys.path.append('/home/yuncong/Brain/preprocess/morphsnakes')\n",
    "import morphsnakes\n",
    "\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "volume_dir = '/oasis/projects/nsf/csd395/yuncong/CSHL_volumes/'\n",
    "scoremaps_rootdir = '/home/yuncong/csd395/CSHL_scoremaps_lossless/'\n",
    "autoAnnotations_rootdir = '/oasis/projects/nsf/csd395/yuncong/CSHL_autoAnnotations_snake'\n",
    "autoAnnotationViz_rootdir = '/oasis/projects/nsf/csd395/yuncong/CSHL_autoAnnotationsViz_snake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stack = 'MD593'\n",
    "\n",
    "autoAnnotations_dir = autoAnnotations_rootdir + '/' + stack\n",
    "if not os.path.exists(autoAnnotations_dir):\n",
    "    os.makedirs(autoAnnotations_dir)\n",
    "\n",
    "autoAnnotationViz_dir = autoAnnotationViz_rootdir + '/' + stack\n",
    "if not os.path.exists(autoAnnotationViz_dir):\n",
    "    os.makedirs(autoAnnotationViz_dir)\n",
    "\n",
    "first_bs_sec, last_bs_sec = section_range_lookup[stack]\n",
    "first_detect_sec, last_detect_sec = detect_bbox_range_lookup[stack]\n",
    "\n",
    "dm = DataManager(stack=stack)\n",
    "\n",
    "# labels_to_detect = ['5N', '7n', '7N', '12N', 'Gr', 'LVe', 'Pn', 'SuVe', 'VLL']\n",
    "# labels_to_detect = ['7n']\n",
    "\n",
    "labels_to_detect = ['5N', '7n', '7N', '12N', 'Gr', 'LVe', 'Pn', 'SuVe', 'VLL', \n",
    "                     '6N', 'Amb', 'R', 'Tz', 'Sol', 'RtTg', 'LRt', 'LC', 'AP', 'sp5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "section_contains_label = {}\n",
    "# for sec in range(first_bs_sec, last_bs_sec+1):\n",
    "#     dm = DataManager(stack=stack, section=sec)\n",
    "#     try:\n",
    "#         user, ts, _, res = dm.load_proposal_review_result('yuncong', 'latest', 'consolidated')\n",
    "#         section_contains_label[sec] = set([lm['label'] for lm in res])\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_contour_points(labelmap):\n",
    "    '''\n",
    "    return is (x,y)\n",
    "    '''\n",
    "\n",
    "    regions = regionprops(labelmap)\n",
    "\n",
    "    contour_points = {}\n",
    "\n",
    "    for r in regions:\n",
    "\n",
    "        (min_row, min_col, max_row, max_col) = r.bbox\n",
    "\n",
    "        padded = np.pad(r.filled_image, ((5,5),(5,5)), mode='constant', constant_values=0)\n",
    "\n",
    "        contours = find_contours(padded, .5, fully_connected='high')\n",
    "        contours = [cnt.astype(np.int) for cnt in contours if len(cnt) > 10]\n",
    "        if len(contours) > 0:\n",
    "#             if len(contours) > 1:\n",
    "#                 sys.stderr.write('%d: region has more than one part\\n' % r.label)\n",
    "                \n",
    "            contours = sorted(contours, key=lambda c: len(c), reverse=True)\n",
    "            contours_list = [c-(5,5) for c in contours]\n",
    "            contour_points[r.label] = sorted([c[np.arange(0, c.shape[0], 10)][:, ::-1] + (min_col, min_row) \n",
    "                                for c in contours_list], key=lambda c: len(c), reverse=True)\n",
    "            \n",
    "        elif len(contours) == 0:\n",
    "#             sys.stderr.write('no contour is found\\n')\n",
    "            continue\n",
    "\n",
    "    #         viz = np.zeros_like(r.filled_image)\n",
    "    #         viz[pts_sampled[:,0], pts_sampled[:,1]] = 1\n",
    "    #         plt.imshow(viz, cmap=plt.cm.gray);\n",
    "    #         plt.show();\n",
    "        \n",
    "    return contour_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for sec in range(first_detect_sec, last_detect_sec+1):\n",
    "for sec in [150]:\n",
    "    \n",
    "    scoremaps_dir = os.path.join(scoremaps_rootdir, stack, '%04d'%sec)\n",
    "    \n",
    "    print '\\n'\n",
    "    print sec\n",
    "    \n",
    "    #########################\n",
    "    \n",
    "    ref_sections = {}\n",
    "    init_annotations = {}\n",
    "\n",
    "    for l in labels_to_detect:\n",
    "\n",
    "        nbrs_containing_this_label = [(abs(nbr-sec), nbr) for i, nbr in enumerate(range(sec-3,sec)+range(sec+1,sec+4)) \n",
    "                if nbr in section_contains_label and l in section_contains_label[nbr]]\n",
    "\n",
    "        if len(nbrs_containing_this_label) == 0:\n",
    "            sys.stderr.write('No neighbor within three sections has annotation of class %s.\\n' % l)\n",
    "            continue\n",
    "\n",
    "        closest_nbr = sorted(nbrs_containing_this_label)[0][1]\n",
    "\n",
    "        dm_nbr = DataManager(stack=stack, section=closest_nbr)\n",
    "        user, ts, _, res = dm_nbr.load_proposal_review_result('yuncong', 'latest', 'consolidated')\n",
    "\n",
    "        lms = [lm for lm in res if lm['label'] == l]\n",
    "        if len(lms) > 1:\n",
    "            sys.stderr.write('Region %s has more than one part.\\n' % l)\n",
    "            # use the larger one\n",
    "            init_annotations[l] = sorted([(Polygon(lm['vertices']).area, lm) for lm in lms])[-1][1]\n",
    "        else:\n",
    "            assert len(lms) == 1\n",
    "            init_annotations[l] = lms[0]\n",
    "        \n",
    "        ref_sections[l] = closest_nbr\n",
    "\n",
    "    #########################\n",
    "\n",
    "    valid_labels = set(labels_to_detect) & set(init_annotations.keys())\n",
    "    print valid_labels\n",
    "    \n",
    "    if len(valid_labels) == 0:\n",
    "        sys.stderr.write('No valid labels exist.\\n')\n",
    "        continue\n",
    "\n",
    "    new_res = []\n",
    "    \n",
    "    for l in valid_labels:\n",
    "#     for l in ['7N']:\n",
    "        \n",
    "        print l\n",
    "        \n",
    "        try:\n",
    "            scoremap_roi = bp.unpack_ndarray_file(os.path.join(scoremaps_dir, \n",
    "                                                       '%(stack)s_%(sec)04d_roi1_denseScoreMapLossless_%(label)s.bp' % \\\n",
    "                                                       {'stack': stack, 'sec': sec, 'label': l}))\n",
    "        except:\n",
    "            sys.stderr.write('No scoremap of %s exists\\n' % (l))\n",
    "            continue\n",
    "        \n",
    "        lm = init_annotations[l]\n",
    "        \n",
    "        init_cnt = np.array(lm['vertices']).copy()\n",
    "        init_cnt = (init_cnt/8).astype(np.int)\n",
    "        \n",
    "#         print init_cnt\n",
    "        \n",
    "#         cx, cy = np.mean(init_cnt, axis=0)\n",
    "#         center = np.array([cx, cy])\n",
    "#         init_cnt =  (.5*(init_cnt - center) + center).astype(np.int)\n",
    "        \n",
    "        dataset = stack + '_' + '%04d'%sec + '_roi1'\n",
    "        \n",
    "        interpolation_xmin, interpolation_xmax, \\\n",
    "        interpolation_ymin, interpolation_ymax = np.loadtxt(os.path.join(scoremaps_dir, \n",
    "                                                                         '%(dataset)s_denseScoreMapLossless_%(label)s_interpBox.txt' % \\\n",
    "                                        {'dataset': dataset, 'label': l})).astype(np.int)\n",
    "        \n",
    "#         dense_score_map_lossless = np.pad(scoremap, ((interpolation_ymin, dm.image_height-interpolation_ymax-1), \n",
    "#                                       (interpolation_xmin, dm.image_width-interpolation_xmax-1)),\n",
    "#                                   mode='constant', constant_values=0)\n",
    "\n",
    "\n",
    "        dense_scoremap_lossless = np.zeros((dm.image_height, dm.image_width), np.float32)\n",
    "        dense_scoremap_lossless[interpolation_ymin:interpolation_ymax+1,\n",
    "                                interpolation_xmin:interpolation_xmax+1] = scoremap_roi\n",
    "        \n",
    "        scoremap = dense_score_map_lossless[::8, ::8]\n",
    "        \n",
    "        plt.figure();\n",
    "        plt.imshow(scoremap, cmap=plt.cm.hot);\n",
    "        plt.show();\n",
    "        \n",
    "#         init_levelset = convert_cnt_to_levelset(init_cnt, levelset_shape=scoremap.shape[:2])\n",
    "        \n",
    "        p = Path(init_cnt)\n",
    "\n",
    "        xmin, ymin = init_cnt.min(axis=0) - 100\n",
    "        xmax, ymax = init_cnt.max(axis=0) + 100\n",
    "        \n",
    "        xmin = max(xmin, 0)\n",
    "        ymin = max(ymin, 0)\n",
    "        xmax = min(xmax, dm.image_width/8-1)\n",
    "        ymax = min(ymax, dm.image_height/8-1)\n",
    "\n",
    "        xs, ys = np.meshgrid(range(xmin, xmax+1), range(ymin, ymax+1))\n",
    "        pts = np.c_[xs.flat, ys.flat]\n",
    "\n",
    "        within = p.contains_points(pts)\n",
    "        within_pts = pts[np.where(within)[0]]\n",
    "\n",
    "        init_levelset = np.zeros((ymax+1-ymin, xmax+1-xmin))\n",
    "        init_levelset[within_pts[:,1]-ymin, within_pts[:,0]-xmin] = 1.\n",
    "        \n",
    "        msnake = morphsnakes.MorphACWE((scoremap[ymin:ymax+1, xmin:xmax+1] > 0.001).astype(np.float), \n",
    "                                       smoothing=3, lambda1=1, lambda2=1)\n",
    "        \n",
    "        msnake.levelset = init_levelset.copy()\n",
    "\n",
    "        diffs = []\n",
    "        for i in range(100): \n",
    "            if i > 1:\n",
    "                diff = np.count_nonzero(msnake.levelset - prev_levelset != 0)\n",
    "                if i > 10:\n",
    "                    diffs.append(diff)\n",
    "                    if np.abs(np.mean(diffs[-5:]) - np.mean(diffs[-10:-5])) < 2:\n",
    "                        break\n",
    "            prev_levelset = msnake.levelset.copy()\n",
    "\n",
    "            msnake.step()\n",
    "            \n",
    "#             plt.figure();\n",
    "#             plt.imshow(msnake.levelset);\n",
    "#             plt.show();\n",
    "            \n",
    "        print i\n",
    "\n",
    "        try:\n",
    "            new_cnts = find_contours(msnake.levelset, 0.5)\n",
    "            if len(new_cnts) > 1:\n",
    "                sys.stderr.write('More than one contour are detected from snake levelset: ' + \\\n",
    "                                ('%d '*len(new_cnts)+'\\n') % tuple([c.shape[0] for c in new_cnts]))\n",
    "#                 new_cnt = sorted([(len(c),c[:,::-1].astype(np.int)) for c in new_cnts])[-1][1]\n",
    "                new_cnt = np.vstack([c[:, ::-1] for c in new_cnts])\n",
    "            else:\n",
    "                new_cnt = new_cnts[0][:, ::-1].astype(np.int)\n",
    "        except:\n",
    "            sys.stderr.write('No contour detected from snake levelset.\\n')\n",
    "            continue\n",
    "        \n",
    "        new_cnt = new_cnt + (xmin, ymin)\n",
    "        new_cnt_subsampled = new_cnt[::20].copy()\n",
    "        \n",
    "#         n_sample_points = Polygon(new_cnt).exterior.length / 20\n",
    "#         new_cnt = new_cnt[np.linspace(0, new_cnt.shape[0]-1, n_sample_points).astype(np.int)]\n",
    "                \n",
    "        new_lm = lm.copy()\n",
    "        new_lm['vertices'] = new_cnt_subsampled.astype(np.int) * 8\n",
    "        new_lm['refVertices'] = np.array(lm['vertices']).copy()\n",
    "                            \n",
    "        new_res.append(new_lm)\n",
    "        \n",
    "    ######################################\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%m%d%Y%H%M%S\")\n",
    "\n",
    "    autoAnnotation_filepath = autoAnnotations_dir + '/%(stack)s_%(sec)04d_autoAnnotate_%(timestamp)s_consolidated.pkl' % \\\n",
    "                        {'stack': stack, 'sec': sec, 'timestamp': timestamp}\n",
    "\n",
    "#     pickle.dump(new_res, open(autoAnnotation_filepath, 'w'))   \n",
    "        \n",
    "    ######################################\n",
    "    \n",
    "    \n",
    "    dm = DataManager(stack=stack, section=sec)\n",
    "    dm._load_image(versions=['rgb-jpg'])\n",
    "    cropped_img = dm.image_rgb_jpg[::8, ::8]\n",
    "\n",
    "    viz = cropped_img.copy()\n",
    "\n",
    "    for lm in new_res:\n",
    "\n",
    "    # for pt in init_cnt:\n",
    "    #     cv2.circle(viz, tuple(pt), 2, (0,255,0,255), -1)\n",
    "\n",
    "        ref_cnt = np.array(lm['refVertices']).astype(np.int)/8\n",
    "        for xy in ref_cnt:\n",
    "            cv2.circle(viz, tuple(xy), 3, (0,255,0), -1)\n",
    "\n",
    "        new_cnt = np.array(lm['vertices'])/8\n",
    "                \n",
    "        for xy in new_cnt:\n",
    "            cv2.circle(viz, tuple(xy), 5, (255,0,0), -1)\n",
    "        cv2.polylines(viz, [new_cnt.astype(np.int)], True, (255,0,0), 2)\n",
    "\n",
    "\n",
    "        if sec in section_contains_label and lm['label'] in section_contains_label[sec]:\n",
    "            dm = DataManager(stack=stack, section=sec)\n",
    "            user, ts, _, res = dm.load_proposal_review_result('yuncong', 'latest', 'consolidated')\n",
    "            manual_cnt = [x['vertices'] for x in res if x['label'] == lm['label']][0]\n",
    "            manual_cnt = np.array(manual_cnt)/8\n",
    "\n",
    "            for xy in manual_cnt.astype(np.int):\n",
    "                cv2.circle(viz, tuple(xy), 3, (0,0,255), -1)\n",
    "            cv2.polylines(viz, [manual_cnt.astype(np.int)], True, (0,0,255), 2)\n",
    "\n",
    "\n",
    "        lx, ly = np.array(lm['labelPos'])/8\n",
    "        cv2.putText(viz, lm['label'], (int(lx)-10, int(ly)+10), cv2.FONT_HERSHEY_DUPLEX, 1, ((0,0,0)), 3)\n",
    "\n",
    "#     cv2.imwrite(autoAnnotationViz_dir + '/%(stack)s_%(sec)04d_autoAnnotationViz.jpg' % \\\n",
    "#                 {'stack': stack, 'sec': sec}, viz[..., ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20));\n",
    "plt.imshow(viz);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_volume_atlas_projected = bp.unpack_ndarray_file(volume_dir + '/%(stack)s_volume_atlasProjected.bp'%{'stack':stack})\n",
    "print test_volume_atlas_projected.shape\n",
    "\n",
    "(volume_xmin, volume_xmax, volume_ymin, volume_ymax, volume_zmin, volume_zmax) = \\\n",
    "    np.loadtxt(os.path.join(volume_dir, 'volume_%(stack)s_scoreMap_limits.txt' % {'stack': stack}), dtype=np.int)\n",
    "    \n",
    "# labels = ['BackG', '5N', '7n', '7N', '12N', 'Gr', 'LVe', 'Pn', 'SuVe', 'VLL']\n",
    "labels = ['BackG', '5N', '7n', '7N', '12N', 'Gr', 'LVe', 'Pn', 'SuVe', 'VLL', \n",
    "                     '6N', 'Amb', 'R', 'Tz', 'Sol', 'RtTg', 'LRt', 'LC', 'AP', 'sp5']\n",
    "\n",
    "label_dict = dict([(l,i) for i, l in enumerate(labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "downsample_factor = 16\n",
    "\n",
    "section_thickness = 20 # in um\n",
    "xy_pixel_distance_lossless = 0.46\n",
    "xy_pixel_distance_tb = xy_pixel_distance_lossless * 32 # in um, thumbnail\n",
    "# factor = section_thickness/xy_pixel_distance_lossless\n",
    "\n",
    "xy_pixel_distance_downsampled = xy_pixel_distance_lossless * downsample_factor\n",
    "z_xy_ratio_downsampled = section_thickness / xy_pixel_distance_downsampled\n",
    "    \n",
    "# build annotation volume\n",
    "section_bs_begin, section_bs_end = section_range_lookup[stack]\n",
    "print section_bs_begin, section_bs_end\n",
    "\n",
    "\n",
    "map_z_to_section = {}\n",
    "for s in range(section_bs_begin, section_bs_end+1):\n",
    "    for z in range(int(z_xy_ratio_downsampled*s) - volume_zmin, \n",
    "                   int(z_xy_ratio_downsampled*(s+1)) - volume_zmin + 1):\n",
    "        map_z_to_section[z] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.imshow(test_volume_atlas_projected[..., 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for sec in range(first_detect_sec, last_detect_sec+1):\n",
    "# for sec in [139]:\n",
    "    \n",
    "    scoremaps_dir = os.path.join(scoremaps_rootdir, stack, '%04d'%sec)\n",
    "    \n",
    "    print '\\n'\n",
    "    print sec\n",
    "    \n",
    "    #########################\n",
    "        \n",
    "    z_lowerlim = int(z_xy_ratio_downsampled*sec) - volume_zmin\n",
    "    z_upperlim = int(z_xy_ratio_downsampled*(sec+1)) - volume_zmin -1 \n",
    "    projected_annotation_labelmap = test_volume_atlas_projected[..., z_lowerlim]\n",
    "    \n",
    "    init_cnts = find_contour_points(projected_annotation_labelmap) # downsampled 16\n",
    "    \n",
    "    init_cnts = dict([(labels[label_ind], (cnts[0]+(volume_xmin, volume_ymin))*2) \n",
    "                      for label_ind, cnts in init_cnts.iteritems()])\n",
    "    \n",
    "\n",
    "    valid_labels = set(labels_to_detect) & set(init_cnts.keys())\n",
    "    print valid_labels\n",
    "    \n",
    "    if len(valid_labels) == 0:\n",
    "        sys.stderr.write('No valid labels exist.\\n')\n",
    "        continue\n",
    "\n",
    "    new_res = []\n",
    "    \n",
    "    for l in valid_labels:\n",
    "#     for l in ['sp5']:\n",
    "        \n",
    "        print l\n",
    "        \n",
    "        try:\n",
    "            scoremap_roi = bp.unpack_ndarray_file(os.path.join(scoremaps_dir, \n",
    "                                                       '%(stack)s_%(sec)04d_roi1_denseScoreMapLossless_%(label)s.bp' % \\\n",
    "                                                       {'stack': stack, 'sec': sec, 'label': l}))\n",
    "        except:\n",
    "            sys.stderr.write('No scoremap of %s exists\\n' % (l))\n",
    "            continue\n",
    "                        \n",
    "        \n",
    "        dataset = stack + '_' + '%04d'%sec + '_roi1'\n",
    "        \n",
    "        interpolation_xmin, interpolation_xmax, \\\n",
    "        interpolation_ymin, interpolation_ymax = np.loadtxt(os.path.join(scoremaps_dir, \n",
    "                                                                         '%(dataset)s_denseScoreMapLossless_%(label)s_interpBox.txt' % \\\n",
    "                                        {'dataset': dataset, 'label': l})).astype(np.int)\n",
    "        \n",
    "        dense_scoremap_lossless = np.zeros((dm.image_height, dm.image_width), np.float32)\n",
    "        dense_scoremap_lossless[interpolation_ymin:interpolation_ymax+1,\n",
    "                                interpolation_xmin:interpolation_xmax+1] = scoremap_roi\n",
    "        \n",
    "        scoremap = dense_scoremap_lossless[::8, ::8]\n",
    "        scoremap_viz = img_as_ubyte(plt.cm.hot(scoremap)[..., :3])\n",
    "                \n",
    "#         plt.figure();\n",
    "#         plt.imshow(scoremap, cmap=plt.cm.hot);\n",
    "#         plt.show();\n",
    "        \n",
    "#         init_levelset = convert_cnt_to_levelset(init_cnt, levelset_shape=scoremap.shape[:2])\n",
    "        \n",
    "        init_cnt = init_cnts[l]\n",
    "        \n",
    "#         cx, cy = np.mean(init_cnt, axis=0)\n",
    "#         center = np.array([cx, cy])\n",
    "#         init_cnt =  (2.*(init_cnt - center) + center).astype(np.int)\n",
    "    \n",
    "        p = Path(init_cnt)\n",
    "\n",
    "        xmin, ymin = init_cnt.min(axis=0) - 100\n",
    "        xmax, ymax = init_cnt.max(axis=0) + 100\n",
    "        \n",
    "        xmin = max(xmin, 0)\n",
    "        ymin = max(ymin, 0)\n",
    "        xmax = min(xmax, dm.image_width/8-1)\n",
    "        ymax = min(ymax, dm.image_height/8-1)\n",
    "\n",
    "        xs, ys = np.meshgrid(range(xmin, xmax+1), range(ymin, ymax+1))\n",
    "        pts = np.c_[xs.flat, ys.flat]\n",
    "\n",
    "        within = p.contains_points(pts)\n",
    "        within_pts = pts[np.where(within)[0]]\n",
    "\n",
    "        init_levelset = np.zeros((ymax+1-ymin, xmax+1-xmin))\n",
    "        init_levelset[within_pts[:,1]-ymin, within_pts[:,0]-xmin] = 1.\n",
    "        \n",
    "        scoremap_roi = scoremap[ymin:ymax+1, xmin:xmax+1] \n",
    "        \n",
    "        msnake = morphsnakes.MorphACWE((scoremap_roi > 0.1).astype(np.float), \n",
    "                                       smoothing=3, lambda1=1., lambda2=1.)\n",
    "        \n",
    "        msnake.levelset = init_levelset.copy()\n",
    "        # levelset values are either 1.0 or 0.0\n",
    "        \n",
    "        dq = deque([None, None])\n",
    "        for i in range(1000): \n",
    "            \n",
    "            # at stable stage, the levelset (thus contour) will oscilate, \n",
    "            # so instead of comparing to previous levelset, must compare to the one before the previous\n",
    "            oneBefore_levelset = dq.popleft()\n",
    "            \n",
    "            if i > 10:\n",
    "#                 print np.count_nonzero(msnake.levelset - oneBefore_levelset)\n",
    "                if np.count_nonzero(msnake.levelset - oneBefore_levelset) < 3:\n",
    "                    break\n",
    "\n",
    "            dq.append(msnake.levelset)\n",
    "        \n",
    "            msnake.step()\n",
    "            \n",
    "#             if i % 10 == 0:\n",
    "#                 a = scoremap_viz[ymin:ymax+1, xmin:xmax+1].copy()\n",
    "#                 cnts = find_contours(msnake.levelset, .5)\n",
    "#                 for cnt in cnts:\n",
    "#                     for c in cnt[:,::-1]:\n",
    "#                         cv2.circle(a, tuple(c.astype(np.int)), 1, (0,255,0), -1)\n",
    "\n",
    "#                 plt.figure(figsize=(10,10));\n",
    "#                 plt.imshow(a);\n",
    "#                 plt.show();\n",
    "\n",
    "        # in the final levelset, inside could be 0. or 1., hard to say        \n",
    "        edge_arr = np.r_[msnake.levelset[:,0], msnake.levelset[:,-1], msnake.levelset[0], msnake.levelset[-1]]        \n",
    "        pos_edge_num = np.count_nonzero(edge_arr)\n",
    "        bool_arr = msnake.levelset.astype(np.bool)\n",
    "        \n",
    "        if pos_edge_num < len(edge_arr) - pos_edge_num:\n",
    "            print 'mean inside score:', scoremap_roi[bool_arr].mean()\n",
    "        else:\n",
    "            print 'mean inside score:', scoremap_roi[~bool_arr].mean()\n",
    "        print 'area:', np.count_nonzero(bool_arr)\n",
    "        print 'snake iteration:', i\n",
    "\n",
    "        try:\n",
    "            new_cnts = find_contours(msnake.levelset, 0.5)\n",
    "            if len(new_cnts) > 1:\n",
    "                sys.stderr.write('More than one contour are detected from snake levelset: ' + \\\n",
    "                                ('%d '*len(new_cnts)+'\\n') % tuple([c.shape[0] for c in new_cnts]))\n",
    "#                 new_cnt = sorted([(len(c),c[:,::-1].astype(np.int)) for c in new_cnts])[-1][1]\n",
    "                new_cnt = np.vstack([c[:, ::-1] for c in new_cnts])\n",
    "            else:\n",
    "                new_cnt = new_cnts[0][:, ::-1].astype(np.int)\n",
    "        except:\n",
    "            sys.stderr.write('No contour detected from snake levelset.\\n')\n",
    "            continue\n",
    "        \n",
    "        new_cnt = new_cnt + (xmin, ymin)\n",
    "        new_cnt_subsampled = new_cnt[::20].copy()\n",
    "                \n",
    "#         n_sample_points = Polygon(new_cnt).exterior.length / 20\n",
    "#         new_cnt = new_cnt[np.linspace(0, new_cnt.shape[0]-1, n_sample_points).astype(np.int)]\n",
    "                \n",
    "#         new_lm = lm.copy()\n",
    "        new_lm = {}\n",
    "        new_lm['label'] = l\n",
    "        new_lm['labelPos'] = new_cnt_subsampled.mean(axis=0)\n",
    "        new_lm['vertices'] = new_cnt_subsampled.astype(np.int) * 8\n",
    "        new_lm['refVertices'] = np.array(init_cnt).copy()\n",
    "                            \n",
    "        new_res.append(new_lm)\n",
    "        \n",
    "    ######################################\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%m%d%Y%H%M%S\")\n",
    "\n",
    "    autoAnnotation_filepath = autoAnnotations_dir + '/%(stack)s_%(sec)04d_autoAnnotate_%(timestamp)s_consolidated.pkl' % \\\n",
    "                        {'stack': stack, 'sec': sec, 'timestamp': timestamp}\n",
    "\n",
    "#     pickle.dump(new_res, open(autoAnnotation_filepath, 'w'))   \n",
    "        \n",
    "    ######################################\n",
    "    \n",
    "    \n",
    "    dm = DataManager(stack=stack, section=sec)\n",
    "    dm._load_image(versions=['rgb-jpg'])\n",
    "    cropped_img = dm.image_rgb_jpg[::8, ::8]\n",
    "\n",
    "    viz = cropped_img.copy()\n",
    "    \n",
    "    \n",
    "    for l in valid_labels:\n",
    "        ref_cnt = init_cnts[l].astype(np.int)\n",
    "        for xy in ref_cnt:\n",
    "            cv2.circle(viz, tuple(xy), 3, (0,255,0), -1)\n",
    "            \n",
    "    for lm in new_res:\n",
    "\n",
    "    # for pt in init_cnt:\n",
    "    #     cv2.circle(viz, tuple(pt), 2, (0,255,0,255), -1)\n",
    "\n",
    "#         ref_cnt = lm['refVertices'].astype(np.int)\n",
    "#         for xy in ref_cnt:\n",
    "#             cv2.circle(viz, tuple(xy), 3, (0,255,0), -1)\n",
    "\n",
    "        new_cnt = np.array(lm['vertices'])/8\n",
    "                \n",
    "        for xy in new_cnt:\n",
    "            cv2.circle(viz, tuple(xy), 5, (255,0,0), -1)\n",
    "        cv2.polylines(viz, [new_cnt.astype(np.int)], True, (255,0,0), 2)\n",
    "\n",
    "\n",
    "#         if sec in section_contains_label and lm['label'] in section_contains_label[sec]:\n",
    "#             dm = DataManager(stack=stack, section=sec)\n",
    "#             user, ts, _, res = dm.load_proposal_review_result('yuncong', 'latest', 'consolidated')\n",
    "#             manual_cnt = [x['vertices'] for x in res if x['label'] == lm['label']][0]\n",
    "#             manual_cnt = np.array(manual_cnt)/8\n",
    "\n",
    "#             for xy in manual_cnt.astype(np.int):\n",
    "#                 cv2.circle(viz, tuple(xy), 3, (0,0,255), -1)\n",
    "#             cv2.polylines(viz, [manual_cnt.astype(np.int)], True, (0,0,255), 2)\n",
    "\n",
    "\n",
    "        lx, ly = np.array(lm['labelPos'])\n",
    "        cv2.putText(viz, lm['label'], (int(lx)-10, int(ly)+10), cv2.FONT_HERSHEY_DUPLEX, 1, ((0,0,0)), 3)\n",
    "\n",
    "    cv2.imwrite(autoAnnotationViz_dir + '/%(stack)s_%(sec)04d_autoAnnotationViz.jpg' % \\\n",
    "                {'stack': stack, 'sec': sec}, viz[..., ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(plt.cm.hot(scoremap_roi));\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def func(sec):\n",
    "    \n",
    "    global dm\n",
    "    \n",
    "    scoremaps_dir = os.path.join(scoremaps_rootdir, stack, '%04d'%sec)\n",
    "    \n",
    "    print '\\n'\n",
    "    print sec\n",
    "    \n",
    "    #########################\n",
    "    \n",
    "    test_volume_atlas_projected = bp.unpack_ndarray_file(volume_dir + '/%(stack)s_volume_atlasProjected.bp'%{'stack':stack})\n",
    "    \n",
    "    z_lowerlim = int(z_xy_ratio_downsampled*sec) - volume_zmin\n",
    "    z_upperlim = int(z_xy_ratio_downsampled*(sec+1)) - volume_zmin -1 \n",
    "    projected_annotation_labelmap = test_volume_atlas_projected[..., z_lowerlim]\n",
    "    \n",
    "    init_cnts = find_contour_points(projected_annotation_labelmap) # downsampled 16\n",
    "    \n",
    "    init_cnts = dict([(labels[label_ind], (cnts[0]+(volume_xmin, volume_ymin))*2) \n",
    "                      for label_ind, cnts in init_cnts.iteritems()])\n",
    "    \n",
    "\n",
    "    valid_labels = set(labels_to_detect) & set(init_cnts.keys())\n",
    "    print valid_labels\n",
    "    \n",
    "    if len(valid_labels) == 0:\n",
    "        sys.stderr.write('No valid labels exist.\\n')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    return\n",
    "    \n",
    "    new_res = []\n",
    "    \n",
    "    for l in valid_labels:\n",
    "        \n",
    "#         print l\n",
    "        \n",
    "        try:\n",
    "            scoremap_roi = bp.unpack_ndarray_file(os.path.join(scoremaps_dir, \n",
    "                                                       '%(stack)s_%(sec)04d_roi1_denseScoreMapLossless_%(label)s.bp' % \\\n",
    "                                                       {'stack': stack, 'sec': sec, 'label': l}))\n",
    "        except:\n",
    "            sys.stderr.write('No scoremap of %s exists\\n' % (l))\n",
    "            continue\n",
    "                        \n",
    "        \n",
    "        dataset = stack + '_' + '%04d'%sec + '_roi1'\n",
    "        \n",
    "        interpolation_xmin, interpolation_xmax, \\\n",
    "        interpolation_ymin, interpolation_ymax = np.loadtxt(os.path.join(scoremaps_dir, \n",
    "                                                                         '%(dataset)s_denseScoreMapLossless_%(label)s_interpBox.txt' % \\\n",
    "                                        {'dataset': dataset, 'label': l})).astype(np.int)\n",
    "        \n",
    "        dense_scoremap_lossless = np.zeros((dm.image_height, dm.image_width), np.float32)\n",
    "        dense_scoremap_lossless[interpolation_ymin:interpolation_ymax+1,\n",
    "                                interpolation_xmin:interpolation_xmax+1] = scoremap_roi\n",
    "        \n",
    "        scoremap = dense_scoremap_lossless[::8, ::8]\n",
    "        scoremap_viz = img_as_ubyte(plt.cm.hot(scoremap)[..., :3])\n",
    "                \n",
    "        init_cnt = init_cnts[l]\n",
    "        \n",
    "        p = Path(init_cnt)\n",
    "\n",
    "        xmin, ymin = init_cnt.min(axis=0) - 100\n",
    "        xmax, ymax = init_cnt.max(axis=0) + 100\n",
    "        \n",
    "        xmin = max(xmin, 0)\n",
    "        ymin = max(ymin, 0)\n",
    "        xmax = min(xmax, dm.image_width/8-1)\n",
    "        ymax = min(ymax, dm.image_height/8-1)\n",
    "\n",
    "        xs, ys = np.meshgrid(range(xmin, xmax+1), range(ymin, ymax+1))\n",
    "        pts = np.c_[xs.flat, ys.flat]\n",
    "\n",
    "        within = p.contains_points(pts)\n",
    "        within_pts = pts[np.where(within)[0]]\n",
    "\n",
    "        init_levelset = np.zeros((ymax+1-ymin, xmax+1-xmin))\n",
    "        init_levelset[within_pts[:,1]-ymin, within_pts[:,0]-xmin] = 1.\n",
    "        \n",
    "        scoremap_roi = scoremap[ymin:ymax+1, xmin:xmax+1] \n",
    "        \n",
    "        msnake = morphsnakes.MorphACWE((scoremap_roi > 0.1).astype(np.float), \n",
    "                                       smoothing=3, lambda1=1., lambda2=1.)\n",
    "        \n",
    "        msnake.levelset = init_levelset.copy()\n",
    "        # levelset values are either 1.0 or 0.0\n",
    "        \n",
    "        dq = deque([None, None])\n",
    "        for i in range(1000): \n",
    "            \n",
    "            # at stable stage, the levelset (thus contour) will oscilate, \n",
    "            # so instead of comparing to previous levelset, must compare to the one before the previous\n",
    "            oneBefore_levelset = dq.popleft()\n",
    "            \n",
    "            if i > 10:\n",
    "                if np.count_nonzero(msnake.levelset - oneBefore_levelset) < 3:\n",
    "                    break\n",
    "\n",
    "            dq.append(msnake.levelset)\n",
    "        \n",
    "            msnake.step()\n",
    "\n",
    "        # in the final levelset, inside could be 0. or 1., hard to say        \n",
    "        edge_arr = np.r_[msnake.levelset[:,0], msnake.levelset[:,-1], msnake.levelset[0], msnake.levelset[-1]]        \n",
    "        pos_edge_num = np.count_nonzero(edge_arr)\n",
    "        bool_arr = msnake.levelset.astype(np.bool)\n",
    "        \n",
    "#         if pos_edge_num < len(edge_arr) - pos_edge_num:\n",
    "#             print 'mean inside score:', scoremap_roi[bool_arr].mean()\n",
    "#         else:\n",
    "#             print 'mean inside score:', scoremap_roi[~bool_arr].mean()\n",
    "#         print 'area:', np.count_nonzero(bool_arr)\n",
    "#         print 'snake iteration:', i\n",
    "\n",
    "        try:\n",
    "            new_cnts = find_contours(msnake.levelset, 0.5)\n",
    "            if len(new_cnts) > 1:\n",
    "#                 sys.stderr.write('More than one contour are detected from snake levelset: ' + \\\n",
    "#                                 ('%d '*len(new_cnts)+'\\n') % tuple([c.shape[0] for c in new_cnts]))\n",
    "                new_cnt = np.vstack([c[:, ::-1] for c in new_cnts])\n",
    "            else:\n",
    "                new_cnt = new_cnts[0][:, ::-1].astype(np.int)\n",
    "        except:\n",
    "#             sys.stderr.write('No contour detected from snake levelset.\\n')\n",
    "            continue\n",
    "        \n",
    "        new_cnt = new_cnt + (xmin, ymin)\n",
    "        new_cnt_subsampled = new_cnt[::20].copy()\n",
    "                \n",
    "        new_lm = {}\n",
    "        new_lm['label'] = l\n",
    "        new_lm['labelPos'] = new_cnt_subsampled.mean(axis=0)\n",
    "        new_lm['vertices'] = new_cnt_subsampled.astype(np.int) * 8\n",
    "        new_lm['refVertices'] = np.array(init_cnt).copy()\n",
    "                            \n",
    "        new_res.append(new_lm)\n",
    "        \n",
    "    ######################################\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%m%d%Y%H%M%S\")\n",
    "\n",
    "    autoAnnotation_filepath = autoAnnotations_dir + '/%(stack)s_%(sec)04d_autoAnnotate_%(timestamp)s_consolidated.pkl' % \\\n",
    "                        {'stack': stack, 'sec': sec, 'timestamp': timestamp}\n",
    "\n",
    "    pickle.dump(new_res, open(autoAnnotation_filepath, 'w'))   \n",
    "        \n",
    "    ######################################\n",
    "    \n",
    "    \n",
    "    dm = DataManager(stack=stack, section=sec)\n",
    "    dm._load_image(versions=['rgb-jpg'])\n",
    "    cropped_img = dm.image_rgb_jpg[::8, ::8]\n",
    "\n",
    "    viz = cropped_img.copy()\n",
    "    \n",
    "    \n",
    "    for l in valid_labels:\n",
    "        ref_cnt = init_cnts[l].astype(np.int)\n",
    "        for xy in ref_cnt:\n",
    "            cv2.circle(viz, tuple(xy), 3, (0,255,0), -1)\n",
    "            \n",
    "    for lm in new_res:\n",
    "\n",
    "        new_cnt = np.array(lm['vertices'])/8\n",
    "                \n",
    "        for xy in new_cnt:\n",
    "            cv2.circle(viz, tuple(xy), 5, (255,0,0), -1)\n",
    "        cv2.polylines(viz, [new_cnt.astype(np.int)], True, (255,0,0), 2)\n",
    "\n",
    "\n",
    "        lx, ly = np.array(lm['labelPos'])\n",
    "        cv2.putText(viz, lm['label'], (int(lx)-10, int(ly)+10), cv2.FONT_HERSHEY_DUPLEX, 1, ((0,0,0)), 3)\n",
    "\n",
    "    cv2.imwrite(autoAnnotationViz_dir + '/%(stack)s_%(sec)04d_autoAnnotationViz.jpg' % \\\n",
    "                {'stack': stack, 'sec': sec}, viz[..., ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sec in range(first_detect_sec, first_detect_sec+2):\n",
    "    func(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "\n",
    "# 360s for one stack\n",
    "# _ = Parallel(n_jobs=16)(delayed(func)(sec) for sec in range(first_detect_sec, last_detect_sec+1))\n",
    "_ = Parallel(n_jobs=2)(delayed(func)(sec) for sec in range(first_detect_sec, first_detect_sec+2))\n",
    "\n",
    "print time.time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
